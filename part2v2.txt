import pickle
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.model_selection import train_test_split
import os
import matplotlib.pyplot as plt
import joblib

# ---------------- Configuration ----------------
visualize = True
training_flag = True
retraining_flag = False
test_cartesian_accuracy_flag = True

# Training hyperparams
predict_horizon = 1            # one-step ahead prediction (k=1). Change with care.
hidden_units = 256
batch_size = 64
epochs = 600
learning_rate = 1e-3
weight_decay = 1e-5
scheduler_step = 100
scheduler_gamma = 0.5
device = torch.device("cpu")   # change to "cuda" if available and desired

# Damping for rollout to reduce drift (0 = no damping, 1 = full trust prediction)
rollout_alpha = 0.9

# ---------------- Model ----------------
class JointAngleRegressor(nn.Module):
    def __init__(self, input_dim=11, hidden_units=256):
        super(JointAngleRegressor, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_units),
            nn.ReLU(),
            nn.Linear(hidden_units, hidden_units),
            nn.ReLU(),
            nn.Linear(hidden_units, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return self.model(x)

# ---------------- Dataset ----------------
class JointDataset(Dataset):
    def __init__(self, X, y):
        self.x = torch.from_numpy(X).float()
        self.y = torch.from_numpy(y).float().reshape(-1, 1)

    def __len__(self):
        return len(self.x)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

# ---------------- Utility: save/load scaler per joint ----------------
def save_scaler(mean, std, path):
    joblib.dump({'mean': mean, 'std': std}, path)

def load_scaler(path):
    return joblib.load(path)

# ----------------- TRAINING PART ----------------- #
if training_flag:
    script_dir = os.path.dirname(os.path.abspath(__file__))

    all_time = []
    all_q = []
    all_goal = []

    # load data_0.pkl .. data_9.pkl
    for i in range(10):
        filename = os.path.join(script_dir, f"data_{i}.pkl")
        if not os.path.isfile(filename):
            print(f"Warning: {filename} not found, skipping.")
            continue

        print(f"Loading {filename} ...")
        with open(filename, 'rb') as f:
            data = pickle.load(f)

        # times
        if 'time' not in data:
            raise KeyError(f"'time' not found in {filename}")
        times = np.array(data['time'])
        all_time.extend(times.tolist())

        # q_mes_all
        if 'q_mes_all' not in data:
            raise KeyError(f"'q_mes_all' not found in {filename}")
        q = np.array(data['q_mes_all'])
        if q.ndim != 2 or q.shape[1] != 7:
            raise ValueError(f"Expect q_mes_all shape (N,7) in {filename}, got {q.shape}")
        all_q.extend(q.tolist())

        # goal: use target_cart_pos_all if exists (per time); else replicate final_target_cart_pos
        if 'target_cart_pos_all' in data:
            g = np.array(data['target_cart_pos_all'])
            if g.ndim != 2 or g.shape[1] != 3 or g.shape[0] != len(times):
                raise ValueError(f"target_cart_pos_all in {filename} must be (N,3) with same N as time")
            all_goal.extend(g.tolist())
        elif 'final_target_cart_pos' in data:
            final_g = np.array(data['final_target_cart_pos'])
            if final_g.shape[0] != 3:
                raise ValueError(f"final_target_cart_pos in {filename} must be length-3 vector")
            replicated = np.tile(final_g.reshape(1,3), (len(times),1))
            all_goal.extend(replicated.tolist())
        elif 'goal_positions' in data:
            g = np.array(data['goal_positions'])
            if g.ndim == 2 and g.shape[1] == 3 and g.shape[0] == len(times):
                all_goal.extend(g.tolist())
            elif g.ndim == 1 and g.shape[0] == 3:
                replicated = np.tile(g.reshape(1,3), (len(times),1))
                all_goal.extend(replicated.tolist())
            else:
                raise ValueError(f"Unhandled goal_positions shape in {filename}: {g.shape}")
        else:
            raise KeyError(f"No goal field ('target_cart_pos_all' or 'final_target_cart_pos') found in {filename}")

    # convert to numpy
    time_array = np.array(all_time)            # (N,)
    q_mes_all = np.array(all_q)               # (N,7)
    goal_positions = np.array(all_goal)       # (N,3)

    print("✅ Combined Data Loaded:")
    print(f"  Total Samples: {time_array.shape[0]}")
    print(f"  q_mes_all Shape: {q_mes_all.shape}")
    print(f"  goal_positions Shape: {goal_positions.shape}")

    if not (len(time_array) == q_mes_all.shape[0] == goal_positions.shape[0]):
        raise ValueError("Loaded arrays have inconsistent lengths")

    # We'll train 7 separate models, each predicts q_j at t+predict_horizon
    N = len(time_array)
    k = predict_horizon
    if N <= k:
        raise RuntimeError("Not enough timesteps relative to predict_horizon")

    train_loaders = []
    test_loaders = []
    scalers = []  # per-joint scaler info (mean/std for X)

    # For each joint: construct X (time, goal, q(t)) and y = q(t+k, joint)
    for joint_idx in range(7):
        # prepare X_all and y_all with horizon shift
        X_all = np.hstack((
            time_array.reshape(-1,1),
            goal_positions,
            q_mes_all
        ))  # shape (N,11)

        # trim last k rows for inputs, first k rows for labels
        X_inputs = X_all[:-k, :]           # shape (N-k,11)
        y_targets = q_mes_all[k:, joint_idx]  # shape (N-k,)

        # train/test split
        X_train, X_test, y_train, y_test = train_test_split(
            X_inputs, y_targets, train_size=0.8, shuffle=True
        )

        # normalize features using training stats (per-joint scaler)
        mean_X = X_train.mean(axis=0)
        std_X = X_train.std(axis=0) + 1e-9
        X_train_norm = (X_train - mean_X) / std_X
        X_test_norm = (X_test - mean_X) / std_X

        # save scaler
        scaler_path = os.path.join(script_dir, f"scaler_joint{joint_idx+1}.pkl")
        save_scaler = lambda m, s, p: joblib.dump({'mean': m, 'std': s}, p)
        save_scaler(mean_X, std_X, scaler_path)
        scalers.append(scaler_path)

        train_dataset = JointDataset(X_train_norm, y_train)
        test_dataset = JointDataset(X_test_norm, y_test)

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        train_loaders.append(train_loader)
        test_loaders.append(test_loader)

        print(f"Prepared joint {joint_idx+1}: X_train {X_train.shape} -> X_train_norm {X_train_norm.shape}")

    # Train per-joint networks
    all_losses = {i: {'train_losses': [], 'test_losses': []} for i in range(7)}

    for joint_idx in range(7):
        model_filename = os.path.join(script_dir, f'neuralq{joint_idx+1}.pt')
        if os.path.isfile(model_filename) and not retraining_flag:
            print(f"✅ {model_filename} exists, skipping training for joint {joint_idx+1}")
            continue

        print(f"\n=== Training Joint {joint_idx+1} ===")
        model = JointAngleRegressor(input_dim=11, hidden_units=hidden_units).to(device)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)

        train_loader = train_loaders[joint_idx]
        test_loader = test_loaders[joint_idx]

        train_losses = []
        test_losses = []

        for epoch in range(epochs):
            model.train()
            running_loss = 0.0
            for Xb, yb in train_loader:
                Xb = Xb.to(device)
                yb = yb.to(device)

                optimizer.zero_grad()
                out = model(Xb)
                loss = criterion(out, yb)
                loss.backward()
                optimizer.step()
                running_loss += loss.item() * Xb.size(0)

            scheduler.step()
            train_loss = running_loss / (len(train_loader.dataset))
            train_losses.append(train_loss)

            # eval
            model.eval()
            test_loss_acc = 0.0
            with torch.no_grad():
                for Xb, yb in test_loader:
                    Xb = Xb.to(device)
                    yb = yb.to(device)
                    out = model(Xb)
                    test_loss_acc += criterion(out, yb).item() * Xb.size(0)
            test_loss = test_loss_acc / (len(test_loader.dataset))
            test_losses.append(test_loss)

            if epoch % 20 == 0 or epoch == epochs-1:
                print(f"Epoch {epoch+1}/{epochs} | TrainLoss: {train_loss:.6e} | TestLoss: {test_loss:.6e}")

        # save model and losses
        torch.save(model.state_dict(), model_filename)
        all_losses[joint_idx]['train_losses'] = train_losses
        all_losses[joint_idx]['test_losses'] = test_losses
        print(f"Saved model: {model_filename}")

    # Save all_losses
    with open(os.path.join(script_dir, "all_losses.pkl"), "wb") as f:
        pickle.dump(all_losses, f)
    print("Saved all_losses.pkl")

    # Optional visualization of losses
    if visualize:
        plt.figure(figsize=(12, 6))
        for j in range(7):
            tl = all_losses[j]['train_losses']
            tr = all_losses[j]['test_losses']
            plt.plot(tl, label=f'J{j+1}_train')
            plt.plot(tr, linestyle='--', label=f'J{j+1}_test')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.title('Training & Test Loss per Joint')
        plt.tight_layout()
        plt.savefig(os.path.join(script_dir, "loss_curves.png"))
        plt.show()

# ----------------- TEST / ROLLOUT ----------------- #
if test_cartesian_accuracy_flag:
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # Load time range to make test horizon
    all_time = []
    for i in range(10):
        filename = os.path.join(script_dir, f"data_{i}.pkl")
        if not os.path.isfile(filename):
            continue
        with open(filename, 'rb') as f:
            data = pickle.load(f)
            all_time.extend(data['time'])
    if len(all_time) == 0:
        raise RuntimeError("No time data found in data_*.pkl")
    time_array = np.array(all_time)
    print(f"Loaded total time samples: {len(time_array)}")

    # load models and scalers
    models = []
    scalers = []
    for j in range(7):
        model = JointAngleRegressor(input_dim=11, hidden_units=hidden_units)
        model_path = os.path.join(script_dir, f'neuralq{j+1}.pt')
        if not os.path.isfile(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}. Please train first.")
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
        models.append(model)

        scaler_path = os.path.join(script_dir, f"scaler_joint{j+1}.pkl")
        if not os.path.isfile(scaler_path):
            raise FileNotFoundError(f"Scaler file not found: {scaler_path}")
        scalers.append(load_scaler(scaler_path))

    # goal sampling
    goal_position_bounds = {'x': (0.4, 0.5), 'y': (-0.2, 0.2), 'z': (0.1, 0.1)}
    number_of_goal_positions_to_test = 10
    goal_positions = [
        [np.random.uniform(*goal_position_bounds['x']),
         np.random.uniform(*goal_position_bounds['y']),
         np.random.uniform(*goal_position_bounds['z'])]
        for _ in range(number_of_goal_positions_to_test)
    ]

    test_time_array = np.linspace(time_array.min(), time_array.max(), 100)

    # simulation init
    from simulation_and_control import pb, MotorCommands, PinWrapper, feedback_lin_ctrl, CartesianDiffKin
    conf_file_name = "pandaconfig.json"
    root_dir = os.path.dirname(os.path.abspath(__file__)).replace("tests", "")

    sim = pb.SimInterface(conf_file_name, conf_file_path_ext=root_dir)
    ext_names = np.expand_dims(np.array(sim.getNameActiveJoints()), axis=0)
    dyn_model = PinWrapper(conf_file_name, "pybullet", ext_names, ["pybullet"], False, 0, root_dir)

    controlled_frame_name = "panda_link8"
    init_joint_angles = np.array(sim.GetInitMotorAngles())
    joint_vel_limits = sim.GetBotJointsVelLimit()
    time_step = sim.GetTimeStep()

    # prepare output logging
    output_dir = os.path.join(script_dir, "task2.1", "archive")
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, "goal_position_errors.txt")

    squared_errors = []

    with open(output_file, "w") as file:
        file.write(f"Init joint angles: {init_joint_angles}\n\n")

        for idx_goal, goal in enumerate(goal_positions):
            print(f"\nTesting goal {idx_goal+1}/{len(goal_positions)}: {goal}")
            file.write(f"Goal {idx_goal+1}: {goal}\n")

            # reset sim
            sim.ResetPose()
            current_q = init_joint_angles.copy()

            predicted_q_over_time = np.zeros((len(test_time_array), 7))
            predicted_q_over_time[0, :] = current_q.copy()

            for t in range(len(test_time_array)-1):
                current_time = test_time_array[t]
                # build raw input: [time, goal(3), q_mes(7)]
                raw_x = np.hstack((np.array([current_time]), np.array(goal), current_q)).reshape(1, -1)  # (1,11)

                # Normalize with each joint's scaler (we trained per-joint scalers using same feature ordering).
                # However scalers were saved per-joint; we use the scaler of each joint to normalize the whole input.
                # Note: we used same mean/std for all features when training each joint's model, so load per-joint.
                # We'll normalize input separately for each model (per its mean/std).
                preds = np.zeros(7)
                for j in range(7):
                    s = scalers[j]
                    mean_j = s['mean']
                    std_j = s['std']
                    x_norm = (raw_x - mean_j.reshape(1, -1)) / std_j.reshape(1, -1)
                    x_tensor = torch.from_numpy(x_norm).float()
                    with torch.no_grad():
                        preds[j] = models[j](x_tensor).numpy().flatten()[0]

                # smoothing (damping) between previous measured current_q and new preds
                next_q = rollout_alpha * preds + (1 - rollout_alpha) * current_q

                # store and advance
                predicted_q_over_time[t+1, :] = next_q.copy()
                current_q = next_q.copy()

                # control: compute qd_des from predicted trajectory if possible
                # Simple finite difference to obtain qd_des for next step
                qd_des = (predicted_q_over_time[t+1, :] - predicted_q_over_time[t, :]) / (test_time_array[1] - test_time_array[0])
                qd_des = np.clip(qd_des, -np.array(joint_vel_limits), np.array(joint_vel_limits))

                # get measured state from sim to compute control command
                q_mes = sim.GetMotorAngles(0)
                qd_mes = sim.GetMotorVelocities(0)

                tau_cmd = feedback_lin_ctrl(dyn_model, q_mes, qd_mes, predicted_q_over_time[t+1, :], qd_des, kp=1000, kd=100)
                cmd = MotorCommands()
                cmd.SetControlCmd(tau_cmd, ["torque"] * len(q_mes))
                sim.Step(cmd, "torque")

                time.sleep(time_step)

            # after rollout
            final_q = predicted_q_over_time[-1, :]
            final_pos, _ = dyn_model.ComputeFK(final_q, controlled_frame_name)
            err = np.linalg.norm(final_pos - np.array(goal))
            squared_errors.append(err**2)

            print(f"Final predicted cartesian pos: {final_pos} | error: {err:.4f} m")
            file.write(f"Final predicted cartesian pos: {final_pos}\nError: {err}\n\n")

    mse = np.mean(squared_errors)
    print(f"\nMean Squared Error (MSE) from target positions: {mse}")
    print(f"Results saved to {output_file}")

    # visualization
    if visualize:
        pos_errs = [np.sqrt(e) for e in squared_errors]
        plt.figure(figsize=(8,4))
        plt.plot(range(1, len(pos_errs)+1), pos_errs, marker='o')
        plt.xlabel('Goal index')
        plt.ylabel('Position error (m)')
        plt.title('Position error per tested goal')
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'position_errors.png'))
        plt.show()
